---
phase: 11-batch-backtest
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - src/decisiongraph/engine.py
  - tests/test_backtest.py
autonomous: true

must_haves:
  truths:
    - "User can call engine.run_backtest() with list of RFAs and receive BatchBacktestResult"
    - "Backtest stops when max_cases limit reached and returns backtest_incomplete=True"
    - "Backtest stops when max_runtime_ms exceeded and returns backtest_incomplete=True"
    - "Backtest stops when max_cells_touched exceeded and returns backtest_incomplete=True"
    - "Results are sorted by (subject, valid_time, system_time) regardless of input order"
    - "Empty rfa_list returns empty results with backtest_incomplete=False"
  artifacts:
    - path: "src/decisiongraph/engine.py"
      provides: "run_backtest() method"
      contains: "def run_backtest"
    - path: "tests/test_backtest.py"
      provides: "Comprehensive backtest tests"
      min_lines: 150
  key_links:
    - from: "src/decisiongraph/engine.py"
      to: "src/decisiongraph/backtest.py"
      via: "BatchBacktestResult import"
      pattern: "from \\.backtest import"
    - from: "src/decisiongraph/engine.py"
      to: "simulate_rfa method"
      via: "self.simulate_rfa() call per RFA"
      pattern: "self\\.simulate_rfa\\("
---

<objective>
Implement engine.run_backtest() method with bounded execution and comprehensive tests.

Purpose: Complete Phase 11 by integrating batch backtest into Engine with all three execution limits (BAT-02) and deterministic ordering (BAT-03).
Output: Working run_backtest() method in engine.py and test_backtest.py with full coverage of requirements BAT-01, BAT-02, BAT-03.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-batch-backtest/11-RESEARCH.md
@.planning/phases/11-batch-backtest/11-01-SUMMARY.md

@src/decisiongraph/engine.py (Engine class with simulate_rfa)
@src/decisiongraph/backtest.py (BatchBacktestResult from Plan 01)
@src/decisiongraph/anchors.py (ExecutionBudget pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add run_backtest() method to Engine class</name>
  <files>src/decisiongraph/engine.py</files>
  <action>
Add run_backtest() method to Engine class in engine.py:

1. Add imports at top of engine.py:
   ```python
   from .backtest import BatchBacktestResult, _sort_results, _count_cells_in_simulation
   from .anchors import ExecutionBudget
   ```

2. Add 'run_backtest' to __all__ export list

3. Add run_backtest() method to Engine class (after simulate_rfa method):
   ```python
   def run_backtest(
       self,
       rfa_list: List[Dict[str, Any]],
       simulation_spec: Dict[str, Any],
       at_valid_time: str,
       as_of_system_time: str,
       max_cases: int = 1000,
       max_runtime_ms: int = 60000,
       max_cells_touched: int = 100000
   ) -> BatchBacktestResult:
       """
       Run simulations over multiple RFAs (BAT-01, BAT-02, BAT-03).

       Iterates over rfa_list, calling simulate_rfa() for each with the same
       simulation_spec and bitemporal coordinates. Results are collected,
       sorted deterministically, and returned with execution metrics.

       Bounded execution (BAT-02):
       - max_cases: Stop after N RFAs processed
       - max_runtime_ms: Stop after timeout exceeded
       - max_cells_touched: Stop after cumulative cell access limit

       When any limit is exceeded, returns partial results with
       backtest_incomplete=True. This prevents DoS via large batches.

       Args:
           rfa_list: List of RFA dicts to simulate (same format as process_rfa)
           simulation_spec: Shadow cells to inject (same for all RFAs)
           at_valid_time: Valid-time coordinate (ISO 8601 UTC)
           as_of_system_time: System-time coordinate (ISO 8601 UTC)
           max_cases: Max RFAs to process (default 1000, BAT-02)
           max_runtime_ms: Max runtime in ms (default 60000 = 60s, BAT-02)
           max_cells_touched: Max cumulative cells (default 100000, BAT-02)

       Returns:
           BatchBacktestResult with:
           - results: List[SimulationResult] sorted by (subject, valid_time, system_time)
           - backtest_incomplete: True if any limit exceeded
           - cases_processed: Number of RFAs successfully processed
           - runtime_ms: Actual runtime in milliseconds
           - cells_touched: Total cells accessed across all simulations

       Example:
           >>> result = engine.run_backtest(
           ...     rfa_list=[
           ...         {"namespace": "corp.hr", "requester_namespace": "corp.hr",
           ...          "requester_id": "analyst", "subject": "employee:alice"},
           ...         {"namespace": "corp.hr", "requester_namespace": "corp.hr",
           ...          "requester_id": "analyst", "subject": "employee:bob"},
           ...     ],
           ...     simulation_spec={"shadow_facts": [...]},
           ...     at_valid_time="2025-01-15T00:00:00Z",
           ...     as_of_system_time="2025-01-15T00:00:00Z"
           ... )
           >>> print(result.cases_processed)  # 2
           >>> print(result.backtest_incomplete)  # False
       """
       # Handle empty input gracefully (BAT-01 edge case)
       if not rfa_list:
           return BatchBacktestResult(
               results=[],
               backtest_incomplete=False,
               cases_processed=0,
               runtime_ms=0.0,
               cells_touched=0
           )

       # Create execution budget (reuse Phase 10 pattern for cases + time)
       budget = ExecutionBudget(max_attempts=max_cases, max_runtime_ms=max_runtime_ms)

       results: List[SimulationResult] = []
       cells_touched = 0

       for rfa_dict in rfa_list:
           # Check max_cases and max_runtime_ms limits (BAT-02)
           if budget.is_exceeded():
               return BatchBacktestResult(
                   results=_sort_results(results),  # BAT-03
                   backtest_incomplete=True,
                   cases_processed=len(results),
                   runtime_ms=budget.elapsed_ms(),
                   cells_touched=cells_touched
               )

           # Check max_cells_touched limit (BAT-02)
           if cells_touched >= max_cells_touched:
               return BatchBacktestResult(
                   results=_sort_results(results),
                   backtest_incomplete=True,
                   cases_processed=len(results),
                   runtime_ms=budget.elapsed_ms(),
                   cells_touched=cells_touched
               )

           # Run simulation for this RFA
           sim_result = self.simulate_rfa(
               rfa_dict=rfa_dict,
               simulation_spec=simulation_spec,
               at_valid_time=at_valid_time,
               as_of_system_time=as_of_system_time
           )

           results.append(sim_result)
           budget.increment()

           # Track cells touched for limit check
           cells_touched += _count_cells_in_simulation(sim_result)

       # All cases completed within budget
       return BatchBacktestResult(
           results=_sort_results(results),  # BAT-03: deterministic order
           backtest_incomplete=False,
           cases_processed=len(results),
           runtime_ms=budget.elapsed_ms(),
           cells_touched=cells_touched
       )
   ```

Key implementation notes:
- Reuse ExecutionBudget from anchors.py for max_cases and max_runtime_ms
- Separate check for max_cells_touched (cumulative metric)
- Sort results BEFORE returning (not after appending)
- Handle empty rfa_list with early return (not error)
  </action>
  <verify>
python -c "
from decisiongraph.engine import Engine
from decisiongraph.chain import create_chain

chain = create_chain('test')
engine = Engine(chain)

# Method exists
assert hasattr(engine, 'run_backtest')
print('run_backtest method exists')
"
  </verify>
  <done>Engine.run_backtest() method exists with bounded execution logic</done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive test suite for batch backtest</name>
  <files>tests/test_backtest.py</files>
  <action>
Create tests/test_backtest.py with comprehensive tests:

```python
"""
Tests for batch backtest functionality (Phase 11).

Requirements tested:
- BAT-01: engine.run_backtest() executes simulation over multiple RFAs
- BAT-02: Backtest bounded by limits (max_cases, max_runtime_ms, max_cells_touched)
- BAT-03: Backtest output deterministically ordered
"""

import pytest
from unittest.mock import patch, MagicMock
import time

from decisiongraph import BatchBacktestResult
from decisiongraph.backtest import _sort_results, _count_cells_in_simulation
from decisiongraph.simulation import SimulationResult, DeltaReport
from decisiongraph.engine import Engine
from decisiongraph.chain import create_chain
from decisiongraph.genesis import create_genesis_cell
from decisiongraph.cell import create_fact_cell, get_current_timestamp


# ============================================================================
# BatchBacktestResult Tests
# ============================================================================

class TestBatchBacktestResult:
    """Tests for BatchBacktestResult dataclass."""

    def test_batch_backtest_result_creation(self):
        """BAT-01: BatchBacktestResult can be created with required fields."""
        result = BatchBacktestResult(
            results=[],
            backtest_incomplete=False,
            cases_processed=0,
            runtime_ms=0.0,
            cells_touched=0
        )

        assert result.results == []
        assert result.backtest_incomplete is False
        assert result.cases_processed == 0
        assert result.runtime_ms == 0.0
        assert result.cells_touched == 0

    def test_batch_backtest_result_is_frozen(self):
        """BatchBacktestResult is immutable (frozen dataclass)."""
        result = BatchBacktestResult(
            results=[],
            backtest_incomplete=False,
            cases_processed=0,
            runtime_ms=0.0,
            cells_touched=0
        )

        with pytest.raises(Exception):  # FrozenInstanceError
            result.cases_processed = 10

    def test_batch_backtest_result_to_dict(self):
        """to_dict() produces serializable output."""
        result = BatchBacktestResult(
            results=[],
            backtest_incomplete=True,
            cases_processed=5,
            runtime_ms=1234.5,
            cells_touched=500
        )

        d = result.to_dict()
        assert d['results'] == []
        assert d['backtest_incomplete'] is True
        assert d['cases_processed'] == 5
        assert d['runtime_ms'] == 1234.5
        assert d['cells_touched'] == 500


# ============================================================================
# Helper Function Tests
# ============================================================================

class TestSortResults:
    """Tests for _sort_results() helper (BAT-03)."""

    def test_sort_results_empty_list(self):
        """Empty list returns empty list."""
        assert _sort_results([]) == []

    def test_sort_results_by_subject(self):
        """Results sorted by subject as primary key (BAT-03)."""
        # Create mock SimulationResults with different subjects
        result_b = MagicMock(spec=SimulationResult)
        result_b.rfa_dict = {'subject': 'employee:bob'}
        result_b.at_valid_time = '2025-01-01T00:00:00Z'
        result_b.as_of_system_time = '2025-01-01T00:00:00Z'

        result_a = MagicMock(spec=SimulationResult)
        result_a.rfa_dict = {'subject': 'employee:alice'}
        result_a.at_valid_time = '2025-01-01T00:00:00Z'
        result_a.as_of_system_time = '2025-01-01T00:00:00Z'

        sorted_results = _sort_results([result_b, result_a])

        # alice should come before bob
        assert sorted_results[0].rfa_dict['subject'] == 'employee:alice'
        assert sorted_results[1].rfa_dict['subject'] == 'employee:bob'

    def test_sort_results_by_valid_time(self):
        """Results sorted by valid_time as secondary key (BAT-03)."""
        result_later = MagicMock(spec=SimulationResult)
        result_later.rfa_dict = {'subject': 'employee:alice'}
        result_later.at_valid_time = '2025-02-01T00:00:00Z'
        result_later.as_of_system_time = '2025-01-01T00:00:00Z'

        result_earlier = MagicMock(spec=SimulationResult)
        result_earlier.rfa_dict = {'subject': 'employee:alice'}
        result_earlier.at_valid_time = '2025-01-01T00:00:00Z'
        result_earlier.as_of_system_time = '2025-01-01T00:00:00Z'

        sorted_results = _sort_results([result_later, result_earlier])

        assert sorted_results[0].at_valid_time == '2025-01-01T00:00:00Z'
        assert sorted_results[1].at_valid_time == '2025-02-01T00:00:00Z'

    def test_sort_results_by_system_time(self):
        """Results sorted by system_time as tertiary key (BAT-03)."""
        result_later = MagicMock(spec=SimulationResult)
        result_later.rfa_dict = {'subject': 'employee:alice'}
        result_later.at_valid_time = '2025-01-01T00:00:00Z'
        result_later.as_of_system_time = '2025-01-02T00:00:00Z'

        result_earlier = MagicMock(spec=SimulationResult)
        result_earlier.rfa_dict = {'subject': 'employee:alice'}
        result_earlier.at_valid_time = '2025-01-01T00:00:00Z'
        result_earlier.as_of_system_time = '2025-01-01T00:00:00Z'

        sorted_results = _sort_results([result_later, result_earlier])

        assert sorted_results[0].as_of_system_time == '2025-01-01T00:00:00Z'
        assert sorted_results[1].as_of_system_time == '2025-01-02T00:00:00Z'

    def test_sort_results_missing_subject(self):
        """Missing subject defaults to empty string (stable sort)."""
        result_with = MagicMock(spec=SimulationResult)
        result_with.rfa_dict = {'subject': 'employee:bob'}
        result_with.at_valid_time = '2025-01-01T00:00:00Z'
        result_with.as_of_system_time = '2025-01-01T00:00:00Z'

        result_without = MagicMock(spec=SimulationResult)
        result_without.rfa_dict = {}  # No subject
        result_without.at_valid_time = '2025-01-01T00:00:00Z'
        result_without.as_of_system_time = '2025-01-01T00:00:00Z'

        # Should not crash, empty string '' comes before 'employee:bob'
        sorted_results = _sort_results([result_with, result_without])
        assert sorted_results[0].rfa_dict.get('subject', '') == ''


class TestCountCellsInSimulation:
    """Tests for _count_cells_in_simulation() helper (BAT-02)."""

    def test_count_cells_empty_results(self):
        """Empty results return 0 cells."""
        result = MagicMock(spec=SimulationResult)
        result.base_result = {}
        result.shadow_result = {}

        assert _count_cells_in_simulation(result) == 0

    def test_count_cells_with_facts(self):
        """Counts fact_cell_ids from base and shadow."""
        result = MagicMock(spec=SimulationResult)
        result.base_result = {
            'results': {'fact_cell_ids': ['a', 'b', 'c']},
            'proof': {}
        }
        result.shadow_result = {
            'results': {'fact_cell_ids': ['x', 'y']},
            'proof': {}
        }

        assert _count_cells_in_simulation(result) == 5  # 3 + 2

    def test_count_cells_with_candidates_and_bridges(self):
        """Counts candidate_cell_ids and bridges_used."""
        result = MagicMock(spec=SimulationResult)
        result.base_result = {
            'results': {'fact_cell_ids': ['a']},
            'proof': {'candidate_cell_ids': ['c1', 'c2'], 'bridges_used': ['b1']}
        }
        result.shadow_result = {
            'results': {'fact_cell_ids': []},
            'proof': {'candidate_cell_ids': ['c3'], 'bridges_used': []}
        }

        # base: 1 fact + 2 candidates + 1 bridge = 4
        # shadow: 0 facts + 1 candidate + 0 bridges = 1
        assert _count_cells_in_simulation(result) == 5


# ============================================================================
# Engine.run_backtest() Integration Tests
# ============================================================================

class TestEngineRunBacktest:
    """Integration tests for Engine.run_backtest() (BAT-01, BAT-02, BAT-03)."""

    @pytest.fixture
    def engine_with_facts(self):
        """Create engine with test chain and facts."""
        chain = create_chain('test_graph')

        # Create genesis
        genesis = create_genesis_cell(
            graph_id='test_graph',
            namespace='corp.hr',
            creator='system',
            bootstrap_mode=True
        )
        chain.append(genesis)

        # Add some facts for querying
        ts = get_current_timestamp()
        fact1 = create_fact_cell(
            namespace='corp.hr',
            subject='employee:alice',
            predicate='has_salary',
            object='50000',
            graph_id='test_graph',
            prev_cell_hash=chain.head.cell_id,
            valid_from=ts,
            system_time=ts,
            creator='hr_system',
            bootstrap_mode=True
        )
        chain.append(fact1)

        fact2 = create_fact_cell(
            namespace='corp.hr',
            subject='employee:bob',
            predicate='has_salary',
            object='60000',
            graph_id='test_graph',
            prev_cell_hash=chain.head.cell_id,
            valid_from=ts,
            system_time=ts,
            creator='hr_system',
            bootstrap_mode=True
        )
        chain.append(fact2)

        return Engine(chain), fact1.cell_id, fact2.cell_id, ts

    def test_run_backtest_empty_list(self):
        """BAT-01: Empty rfa_list returns empty results."""
        chain = create_chain('test')
        genesis = create_genesis_cell('test', 'corp', 'system', bootstrap_mode=True)
        chain.append(genesis)
        engine = Engine(chain)

        result = engine.run_backtest(
            rfa_list=[],
            simulation_spec={},
            at_valid_time=get_current_timestamp(),
            as_of_system_time=get_current_timestamp()
        )

        assert result.results == []
        assert result.backtest_incomplete is False
        assert result.cases_processed == 0
        assert result.runtime_ms == 0.0
        assert result.cells_touched == 0

    def test_run_backtest_single_rfa(self, engine_with_facts):
        """BAT-01: Single RFA returns single result."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        result = engine.run_backtest(
            rfa_list=[{
                'namespace': 'corp.hr',
                'requester_namespace': 'corp.hr',
                'requester_id': 'analyst',
                'subject': 'employee:alice'
            }],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts
        )

        assert result.cases_processed == 1
        assert result.backtest_incomplete is False
        assert len(result.results) == 1

    def test_run_backtest_multiple_rfas(self, engine_with_facts):
        """BAT-01: Multiple RFAs return multiple results."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:alice'},
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:bob'},
            ],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts
        )

        assert result.cases_processed == 2
        assert result.backtest_incomplete is False
        assert len(result.results) == 2

    def test_run_backtest_max_cases_limit(self, engine_with_facts):
        """BAT-02: Stops when max_cases reached."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        # Request 5 RFAs but limit to 2
        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': f'employee:user{i}'}
                for i in range(5)
            ],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts,
            max_cases=2
        )

        assert result.cases_processed == 2
        assert result.backtest_incomplete is True
        assert len(result.results) == 2

    def test_run_backtest_max_runtime_limit(self, engine_with_facts):
        """BAT-02: Stops when max_runtime_ms exceeded."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        # Use very short timeout (1ms) - should stop quickly
        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': f'employee:user{i}'}
                for i in range(100)
            ],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts,
            max_runtime_ms=1  # 1ms timeout
        )

        # Should have processed at least 1 but likely stopped early
        assert result.backtest_incomplete is True
        assert result.cases_processed < 100

    def test_run_backtest_max_cells_touched_limit(self, engine_with_facts):
        """BAT-02: Stops when max_cells_touched exceeded."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        # Set very low cell limit
        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:alice'},
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:bob'},
            ],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts,
            max_cells_touched=1  # Very low limit
        )

        # First RFA should process, second may be blocked
        assert result.cases_processed >= 1
        # If cells_touched > 1 after first RFA, incomplete should be True
        if result.cells_touched >= 1:
            assert result.backtest_incomplete is True

    def test_run_backtest_deterministic_ordering(self, engine_with_facts):
        """BAT-03: Results sorted by (subject, valid_time, system_time)."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        # Submit RFAs in reverse order (bob before alice)
        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:bob'},
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:alice'},
            ],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts
        )

        # Results should be sorted: alice before bob
        assert result.results[0].rfa_dict['subject'] == 'employee:alice'
        assert result.results[1].rfa_dict['subject'] == 'employee:bob'

    def test_run_backtest_with_shadow_spec(self, engine_with_facts):
        """BAT-01: Simulation spec applied to all RFAs."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        # Shadow spec modifies alice's salary
        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:alice'},
            ],
            simulation_spec={
                'shadow_facts': [
                    {'base_cell_id': fact1_id, 'object': '75000'}
                ]
            },
            at_valid_time=ts,
            as_of_system_time=ts
        )

        assert result.cases_processed == 1
        # Result should have delta_report (Phase 9)
        assert result.results[0].delta_report is not None

    def test_run_backtest_tracks_runtime(self, engine_with_facts):
        """runtime_ms is tracked accurately."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:alice'},
            ],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts
        )

        # Runtime should be positive and reasonable
        assert result.runtime_ms > 0
        assert result.runtime_ms < 10000  # Less than 10 seconds

    def test_run_backtest_tracks_cells_touched(self, engine_with_facts):
        """cells_touched accumulates across simulations."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:alice'},
                {'namespace': 'corp.hr', 'requester_namespace': 'corp.hr',
                 'requester_id': 'analyst', 'subject': 'employee:bob'},
            ],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts
        )

        # Should have touched cells from both simulations
        assert result.cells_touched >= 0

    def test_run_backtest_result_is_batch_backtest_result(self, engine_with_facts):
        """Return type is BatchBacktestResult."""
        engine, fact1_id, fact2_id, ts = engine_with_facts

        result = engine.run_backtest(
            rfa_list=[],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts
        )

        assert isinstance(result, BatchBacktestResult)


# ============================================================================
# Edge Cases and Regression Tests
# ============================================================================

class TestBacktestEdgeCases:
    """Edge cases and regression tests."""

    def test_rfa_without_subject_field(self):
        """RFAs without subject field sort correctly (default empty string)."""
        chain = create_chain('test')
        genesis = create_genesis_cell('test', 'corp', 'system', bootstrap_mode=True)
        chain.append(genesis)
        engine = Engine(chain)
        ts = get_current_timestamp()

        # RFA without subject
        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp', 'requester_namespace': 'corp', 'requester_id': 'user'},
            ],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts
        )

        assert result.cases_processed == 1
        assert result.backtest_incomplete is False

    def test_default_limits(self):
        """Default limits are reasonable."""
        chain = create_chain('test')
        genesis = create_genesis_cell('test', 'corp', 'system', bootstrap_mode=True)
        chain.append(genesis)
        engine = Engine(chain)
        ts = get_current_timestamp()

        # Without explicit limits, should use defaults
        result = engine.run_backtest(
            rfa_list=[
                {'namespace': 'corp', 'requester_namespace': 'corp', 'requester_id': 'user'},
            ],
            simulation_spec={},
            at_valid_time=ts,
            as_of_system_time=ts
        )

        # Should complete with defaults (1000 cases, 60s, 100000 cells)
        assert result.backtest_incomplete is False
```
  </action>
  <verify>
cd /workspaces/Decisiongraph-core-v1.3/decisiongraph-complete && python -m pytest tests/test_backtest.py -v --tb=short
  </verify>
  <done>test_backtest.py exists with comprehensive tests covering BAT-01, BAT-02, BAT-03</done>
</task>

<task type="auto">
  <name>Task 3: Run full test suite and verify no regressions</name>
  <files></files>
  <action>
Run full test suite to verify:
1. All new backtest tests pass
2. All 868 existing tests still pass (0 regressions)
3. Total test count increased (868 + new backtest tests)

Commands to run:
```bash
cd /workspaces/Decisiongraph-core-v1.3/decisiongraph-complete

# Run all tests
python -m pytest tests/ -v --tb=short

# Count total tests
python -m pytest tests/ --collect-only -q | tail -1
```

If any tests fail:
- Fix the implementation in engine.py or backtest.py
- Do NOT modify existing tests (they define correct behavior)
- Ensure deterministic behavior (same inputs = same outputs)
  </action>
  <verify>
cd /workspaces/Decisiongraph-core-v1.3/decisiongraph-complete && python -m pytest tests/ -q --tb=no && echo "All tests pass"
  </verify>
  <done>All tests pass including new backtest tests (868+ total)</done>
</task>

</tasks>

<verification>
All checks must pass:
```bash
cd /workspaces/Decisiongraph-core-v1.3/decisiongraph-complete

# Engine.run_backtest() exists
python -c "from decisiongraph.engine import Engine; assert hasattr(Engine, 'run_backtest')"

# BatchBacktestResult returned
python -c "
from decisiongraph.engine import Engine
from decisiongraph.chain import create_chain
from decisiongraph.genesis import create_genesis_cell
from decisiongraph import BatchBacktestResult
from decisiongraph.cell import get_current_timestamp

chain = create_chain('test')
genesis = create_genesis_cell('test', 'corp', 'system', bootstrap_mode=True)
chain.append(genesis)
engine = Engine(chain)
ts = get_current_timestamp()

result = engine.run_backtest([], {}, ts, ts)
assert isinstance(result, BatchBacktestResult)
print('run_backtest returns BatchBacktestResult')
"

# All tests pass
python -m pytest tests/ -q --tb=no
```
</verification>

<success_criteria>
- engine.run_backtest() method implemented with bounded execution (BAT-02)
- Empty input handled gracefully (returns empty BatchBacktestResult)
- max_cases, max_runtime_ms, max_cells_touched limits enforced
- Results deterministically sorted by (subject, valid_time, system_time) (BAT-03)
- Comprehensive test coverage in test_backtest.py
- All 868+ tests pass (0 regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/11-batch-backtest/11-02-SUMMARY.md`
</output>
