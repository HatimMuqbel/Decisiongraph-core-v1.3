---
phase: 11-batch-backtest
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/decisiongraph/backtest.py
autonomous: true

must_haves:
  truths:
    - "BatchBacktestResult frozen dataclass exists with results, backtest_incomplete, cases_processed, runtime_ms, cells_touched fields"
    - "_sort_results() returns results sorted by (subject, valid_time, system_time)"
    - "_count_cells_in_simulation() counts cells from base_result and shadow_result proof bundles"
    - "Empty results list handled gracefully (no crash)"
  artifacts:
    - path: "src/decisiongraph/backtest.py"
      provides: "BatchBacktestResult dataclass and helper functions"
      exports: ["BatchBacktestResult", "_sort_results", "_count_cells_in_simulation"]
      min_lines: 80
  key_links:
    - from: "src/decisiongraph/backtest.py"
      to: "src/decisiongraph/simulation.py"
      via: "SimulationResult type import"
      pattern: "from \\.simulation import SimulationResult"
---

<objective>
Create BatchBacktestResult dataclass and helper functions for batch backtesting infrastructure.

Purpose: Establish the data structures and utilities needed for engine.run_backtest() integration in Plan 02.
Output: New backtest.py module with frozen BatchBacktestResult, _sort_results(), and _count_cells_in_simulation() helpers.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-batch-backtest/11-RESEARCH.md

@src/decisiongraph/simulation.py (SimulationResult pattern)
@src/decisiongraph/anchors.py (ExecutionBudget pattern to reuse)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create backtest.py with BatchBacktestResult dataclass</name>
  <files>src/decisiongraph/backtest.py</files>
  <action>
Create new module src/decisiongraph/backtest.py with:

1. Module docstring explaining batch backtest purpose (BAT-01, BAT-02, BAT-03)

2. Imports:
   - from dataclasses import dataclass
   - from typing import List, Dict, Any
   - from .simulation import SimulationResult

3. BatchBacktestResult frozen dataclass (BAT-01):
   ```python
   @dataclass(frozen=True)
   class BatchBacktestResult:
       """Immutable batch backtest result (BAT-01, BAT-02, BAT-03).

       Contains list of SimulationResults plus metadata about execution budget.
       Results are deterministically sorted by (subject, valid_time, system_time).

       Attributes:
           results: List of SimulationResults (sorted deterministically - BAT-03)
           backtest_incomplete: True if any limit exceeded (max_cases, max_runtime_ms, max_cells_touched)
           cases_processed: Number of RFAs processed before stopping
           runtime_ms: Actual batch runtime in milliseconds
           cells_touched: Cumulative cells accessed across all simulations
       """
       results: List[SimulationResult]
       backtest_incomplete: bool
       cases_processed: int
       runtime_ms: float
       cells_touched: int

       def to_dict(self) -> Dict[str, Any]:
           """Convert to serializable dict."""
           return {
               'results': [r.to_dict() for r in self.results],
               'backtest_incomplete': self.backtest_incomplete,
               'cases_processed': self.cases_processed,
               'runtime_ms': self.runtime_ms,
               'cells_touched': self.cells_touched
           }
   ```

4. __all__ export list with BatchBacktestResult

Follow existing patterns from simulation.py and anchors.py for consistent style.
  </action>
  <verify>
python -c "from decisiongraph.backtest import BatchBacktestResult; print('BatchBacktestResult import OK')"
  </verify>
  <done>BatchBacktestResult frozen dataclass exists with all required fields and to_dict() method</done>
</task>

<task type="auto">
  <name>Task 2: Add helper functions for deterministic sorting and cell counting</name>
  <files>src/decisiongraph/backtest.py</files>
  <action>
Add helper functions to backtest.py:

1. _sort_results() function (BAT-03):
   ```python
   def _sort_results(results: List[SimulationResult]) -> List[SimulationResult]:
       """Sort results deterministically (BAT-03).

       Primary key: subject from rfa_dict
       Secondary key: at_valid_time
       Tertiary key: as_of_system_time

       Handles missing subject (defaults to empty string for consistent sorting).

       Args:
           results: List of SimulationResults to sort

       Returns:
           Sorted list (stable sort, reproducible)
       """
       return sorted(
           results,
           key=lambda r: (
               r.rfa_dict.get('subject', ''),  # Primary: subject (default '' if None)
               r.at_valid_time,                # Secondary: valid_time
               r.as_of_system_time             # Tertiary: system_time
           )
       )
   ```

2. _count_cells_in_simulation() function (BAT-02):
   ```python
   def _count_cells_in_simulation(sim_result: SimulationResult) -> int:
       """Count total cells accessed during simulation (BAT-02).

       Used for max_cells_touched limit. Counts cells in:
       - base_result proof_bundle (facts, rules, bridges)
       - shadow_result proof_bundle (facts, rules, bridges)

       Args:
           sim_result: SimulationResult to count cells from

       Returns:
           Total cell count accessed (base + shadow)
       """
       # Count base result cells
       base_results = sim_result.base_result.get('results', {})
       base_cells = len(base_results.get('fact_cell_ids', []))

       base_proof = sim_result.base_result.get('proof', {})
       base_cells += len(base_proof.get('candidate_cell_ids', []))
       base_cells += len(base_proof.get('bridges_used', []))

       # Count shadow result cells
       shadow_results = sim_result.shadow_result.get('results', {})
       shadow_cells = len(shadow_results.get('fact_cell_ids', []))

       shadow_proof = sim_result.shadow_result.get('proof', {})
       shadow_cells += len(shadow_proof.get('candidate_cell_ids', []))
       shadow_cells += len(shadow_proof.get('bridges_used', []))

       return base_cells + shadow_cells
   ```

3. Update __all__ to include helper functions (for testing, but prefix with _ indicates internal):
   - Keep __all__ = ['BatchBacktestResult'] (helpers are internal)
   - Helpers are testable via module import but not public API

Edge cases to handle:
- Empty results list -> sorted() returns []
- SimulationResult with missing proof keys -> .get() with default []
- rfa_dict without 'subject' key -> default to empty string ''
  </action>
  <verify>
python -c "
from decisiongraph.backtest import _sort_results, _count_cells_in_simulation
from decisiongraph.simulation import SimulationResult

# Test _sort_results with empty list
assert _sort_results([]) == []

print('Helper functions OK')
"
  </verify>
  <done>_sort_results() and _count_cells_in_simulation() functions exist and handle edge cases</done>
</task>

<task type="auto">
  <name>Task 3: Add backtest exports to package __init__.py</name>
  <files>src/decisiongraph/__init__.py</files>
  <action>
Update src/decisiongraph/__init__.py to export BatchBacktestResult:

1. Add import:
   ```python
   from .backtest import BatchBacktestResult
   ```

2. Add to __all__ list:
   ```python
   'BatchBacktestResult',
   ```

Place import after simulation imports (following existing pattern of grouping by module).
  </action>
  <verify>
python -c "from decisiongraph import BatchBacktestResult; print('Package export OK')"
  </verify>
  <done>BatchBacktestResult exported from decisiongraph package</done>
</task>

</tasks>

<verification>
All checks must pass:
```bash
# Module imports correctly
python -c "from decisiongraph.backtest import BatchBacktestResult, _sort_results, _count_cells_in_simulation"

# Package exports BatchBacktestResult
python -c "from decisiongraph import BatchBacktestResult"

# BatchBacktestResult is frozen
python -c "
from decisiongraph.backtest import BatchBacktestResult
r = BatchBacktestResult(results=[], backtest_incomplete=False, cases_processed=0, runtime_ms=0.0, cells_touched=0)
try:
    r.results = []
    print('FAIL: Should be frozen')
except Exception as e:
    print('PASS: BatchBacktestResult is frozen')
"

# Existing tests still pass
cd /workspaces/Decisiongraph-core-v1.3/decisiongraph-complete && python -m pytest tests/ -q --tb=no
```
</verification>

<success_criteria>
- backtest.py module exists with BatchBacktestResult frozen dataclass
- _sort_results() handles empty lists and missing subject keys
- _count_cells_in_simulation() counts cells from both base and shadow results
- BatchBacktestResult exported from package __init__.py
- All 868 existing tests pass (0 regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/11-batch-backtest/11-01-SUMMARY.md`
</output>
